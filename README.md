# ğŸ¬ Netflix End-to-End Data Engineering Pipeline

Welcome to my Netflix data engineering project â€” a cloud-based, medallion-style pipeline built to ingest, transform, and analyze Netflix datasets using scalable tools like **Apache Spark**, **Databricks**, **Azure Data Lake**, and **Power BI**. This README outlines every step from data ingestion to business-ready insights, built entirely within the limitations of a student Databricks workspace.

---

## ğŸ§° Tech Stack Overview

| Tool/Platform              | Purpose                                              |
|----------------------------|------------------------------------------------------|
| Apache Spark (PySpark)     | Scalable ETL and transformation                     |
| Azure Databricks           | Notebook development, job orchestration             |
| Delta Lake                 | ACID-compliant data storage layer                   |
| Azure Data Lake Gen2       | Stage-wise blob storage (Bronze â†’ Silver â†’ Gold)    |
| Azure Data Factory         | Metadata ingestion and file movement                |
| Power BI Desktop           | Visualization of refined analytics                  |

---

## ğŸ” Authentication Setup

Due to student account limitations, I couldnâ€™t deploy Unity Catalog or metadata tracking. To access Azure Data Lake containers, I configured authentication manually:

```python
spark.conf.set(
    "fs.azure.account.key.netflixdatalakes.dfs.core.windows.net",
    dbutils.secrets.get(scope="mysecrets", key="netflix_key")
)
```

## ğŸ§± Architecture: Medallion Layers

ğŸ”· Bronze Layer: 
Raw ingestion of Netflix CSV files (titles, credits, logs)
Stored in raw/ container in Azure Data Lake

ğŸ”· Silver Layer: 
Data deduplication, genre enrichment, cast mapping
Transformation using Databricks notebooks
Loaded into silver/ container

ğŸ”· Gold Layer: 
Delta table created with DLT syntax (not deployed due to account limits)
Analysis performed manually by reading Silver-layer Delta tables


## ğŸ” Pipeline & Workflow Breakdown

ğŸ“˜ Azure Data Factory Pipeline

Step 1: GithubMetadata â€” fetches raw metadata from GitHub

Step 2: ValidationGithub â€” verifies structure, filenames, schema

Step 3: ForEachAllTheFiles â€” iterates through dataset files

Step 4: Copy data Github â€” ingests data into ADLS raw/ container

<img width="1123" alt="Screenshot 2025-07-03 at 10 20 14" src="https://github.com/user-attachments/assets/5ad6e082-ae3d-4a58-9d72-696240146f63" />


## ğŸ§ª Databricks Work/ Job Flow

Step 1: Weekday_lookup â€” checks weekday condition

Step 2: IfWeekday â€” executes SilverMasterData if true

Step 3: IfFalseNotebook â€” fallback path for non-weekday logic

Step 4: silvernotebook_iteration â€” iterates Silver transformation steps

<img width="910" alt="Screenshot 2025-07-03 at 09 27 08" src="https://github.com/user-attachments/assets/2f99d51e-afbe-411b-bc2e-21939f13bb34" />


<img width="1308" alt="Screenshot 2025-07-03 at 09 27 39" src="https://github.com/user-attachments/assets/09fc77cc-b9fc-4f21-8abf-515c3754e7c7" />




## ğŸ“‚ Azure Data Lake Structure

Organized into layer-wise containers:

raw/ â†’ unprocessed CSVs

bronze/ â†’ ingested raw data

silver/ â†’ cleaned and enriched data\

gold/ â†’ manually created Delta tables

<img width="1297" alt="Screenshot 2025-07-03 at 09 30 42" src="https://github.com/user-attachments/assets/b5e92237-6c98-4f8d-83cf-8c24a06e8276" />



## ğŸ“’ Data Transformation Notebooks
Tasks include:
Cast normalization and joins
Genre hierarchy transformation
Filtering by country and date

<img width="1308" alt="Screenshot 2025-07-03 at 09 28 39" src="https://github.com/user-attachments/assets/4c7a8807-5f42-4e5f-8b9d-52665af4c452" />



## ğŸ“Š Power BI Integration
Connected via .pbids file from Databricks Marketplace
SQL warehouse queries fetch data from manually created Delta tables

<img width="1308" alt="Screenshot 2025-07-03 at 09 29 19" src="https://github.com/user-attachments/assets/b86e693d-60f1-40c5-9d1e-28d4636dd479" />




## Step-by-Step Setup Guide
```
# Step 1: Clone the repository
git clone https://github.com/dpka09/NetflixDataEngineeringEndtoEnd.git

# Step 2: Upload datasets to 'raw' container in Azure Data Lake

# Step 3: Configure Spark with secret-based access
#         Use spark.conf.set(...) inside Databricks notebooks

# Step 4: Import and run notebooks for each stage (Bronze â†’ Silver)

# Step 5: Trigger Databricks jobs manually or via scheduler

# Step 6: Connect Power BI Desktop via .pbids file to Delta tables

# Step 7: Explore insights and refine transformations as needed
```
